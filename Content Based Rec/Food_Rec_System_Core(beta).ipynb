{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#سیستم توصیه گر پیشنهاد دستور پخت غذا"
      ],
      "metadata": {
        "id": "HUfpninIcWDI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#download crawled dataset form google drive"
      ],
      "metadata": {
        "id": "gcV9cuf5cgHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/images\n",
        "!wget --header 'Host: drive.usercontent.google.com' --user-agent 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:123.0) Gecko/20100101 Firefox/123.0' --header 'Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8' --header 'Accept-Language: en-US,en;q=0.5' --referer 'https://drive.usercontent.google.com/' --header 'Cookie: NID=512=cxpFxcxYHtJXDTmVhW22poOdBMX9hZOmEPPNZxsQdNcz2d2V1IyKxV-lWz-eWx7IsMed5oNhGthbSBJz4wG0iGjnyf1BHWmZ98zOpRSiXCXEBxvnSGoQgmBnMl1MWvDfSB4k_XpFtIQFXPqhQ_EmiGufwZ458QAccgfwI_O3UU8tNwUSD6Cm3e1rvUMGnjHLV9CvgqAsBg; AEC=Ae3NU9NQkR2a7Ndqa_1t-Ywer6Dn3nE8ksvX3Z5_w5yqQ5yZWkhFKehM7Q; 1P_JAR=2024-03-06-06' --header 'Upgrade-Insecure-Requests: 1' --header 'Sec-Fetch-Dest: document' --header 'Sec-Fetch-Mode: navigate' --header 'Sec-Fetch-Site: cross-site' --header 'Sec-Fetch-User: ?1' 'https://drive.usercontent.google.com/download?id=13yTfGRqoVKf-LONaus_QNTJAsgAcw1GU&export=download&confirm=t&uuid=3a7fbafc-5fd4-44f8-bcb2-9b1421e8e7ca' --output-document 'recipes.db'"
      ],
      "metadata": {
        "id": "OlGNlhQ3hHxq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7502552c-bdfe-49f2-ee85-811e809e7253"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-07 05:04:35--  https://drive.usercontent.google.com/download?id=13yTfGRqoVKf-LONaus_QNTJAsgAcw1GU&export=download&confirm=t&uuid=3a7fbafc-5fd4-44f8-bcb2-9b1421e8e7ca\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 172.253.117.132, 2607:f8b0:400e:c05::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|172.253.117.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 382431232 (365M) [application/octet-stream]\n",
            "Saving to: ‘recipes.db’\n",
            "\n",
            "recipes.db          100%[===================>] 364.71M  86.4MB/s    in 4.5s    \n",
            "\n",
            "2024-03-07 05:04:40 (81.5 MB/s) - ‘recipes.db’ saved [382431232/382431232]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###convert Sqlite database to .CSV format\n",
        "####save food'image into other directory for image retrieval task"
      ],
      "metadata": {
        "id": "iZqQ7usZdAis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import io\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "\n",
        "conn = sqlite3.connect('/content/recipes.db')\n",
        "clients = pd.read_sql('SELECT * FROM recipes', conn)\n",
        "clients.to_csv('csvdata.csv', index=False, encoding='utf-8-sig')\n",
        "\n",
        "\n",
        "conn = sqlite3.connect('/content/recipes.db')\n",
        "cursor = conn.cursor()\n",
        "cursor.execute(\"SELECT id, img1 FROM recipes\")\n",
        "rows = cursor.fetchall()\n",
        "\n",
        "for row in rows:\n",
        "    img_id = row[0]\n",
        "    img_data = row[1]\n",
        "    img = Image.open(io.BytesIO(img_data))\n",
        "    img.save(f\"/content/images/{img_id}.png\")\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "mctPlCSziT2f"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('csvdata.csv')\n",
        "df2 = data.head(25)\n",
        "df2.drop('img1', axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "PyvyR40Altwl",
        "outputId": "bb737a16-3e80-45fe-9580-ab4d4a6511e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-117-84e1272eedc5>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df2.drop('img1', axis=1,inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def split_and_unique(categories):\n",
        "    # Split categories based on commas and remove leading/trailing spaces\n",
        "    categories_list = [category.strip() for category in categories.split(',')]\n",
        "    # Remove duplicates and sort the list\n",
        "    unique_categories = sorted(set(categories_list))\n",
        "    # Join the unique categories back into a string separated by commas\n",
        "    return ','.join(unique_categories)\n",
        "\n",
        "# Apply the function to the 'categories' column and overwrite the original values\n",
        "df2['categories'] = df2['categories'].apply(split_and_unique)\n"
      ],
      "metadata": {
        "id": "hnKYffKxi88_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13cb4c71-d190-4649-e344-7750d39ea949"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-160-3b154f1a2268>:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df2['categories'] = df2['categories'].apply(split_and_unique)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### define function to apply some preprocessing operation on dataset"
      ],
      "metadata": {
        "id": "MHdg0qCrfZSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hazm import Normalizer, word_tokenize, stopwords_list\n",
        "import string\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Normalize the text (remove diacritics, etc.)\n",
        "    normalizer = Normalizer()\n",
        "    normalized_text = normalizer.normalize(text)\n",
        "\n",
        "    # Remove punctuation\n",
        "    punctuation_chars = string.punctuation + '،؛؟«»،٪ء'\n",
        "    normalized_text = normalized_text.translate(str.maketrans('', '', punctuation_chars))\n",
        "\n",
        "    # Convert to lowercase\n",
        "    normalized_text = normalized_text.lower()\n",
        "\n",
        "    # Remove stopwords\n",
        "    stopwords = stopwords_list()\n",
        "    words = word_tokenize(normalized_text)\n",
        "    filtered_words = [word for word in words if word not in stopwords]\n",
        "\n",
        "    return filtered_words\n",
        "\n"
      ],
      "metadata": {
        "id": "ZCuykb0Zja8c"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocess the text\n",
        "df2['title'] = df2['title'].apply(preprocess_text)\n",
        "df2['ingredients'] = df2['ingredients'].apply(preprocess_text)\n",
        "df2['instructions'] = df2['instructions'].apply(preprocess_text)\n",
        "\n",
        "word2vec_model = Word2Vec(sentences=df2['title'] + df2['ingredients'] + df2['instructions'] , vector_size=100, window=5, min_count=1, workers=4)\n"
      ],
      "metadata": {
        "id": "x3KDGshclLPb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "819fd3b6-c5cf-44e7-cce7-058faf304552"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-162-1fcd14a4efb6>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df2['title'] = df2['title'].apply(preprocess_text)\n",
            "<ipython-input-162-1fcd14a4efb6>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df2['ingredients'] = df2['ingredients'].apply(preprocess_text)\n",
            "<ipython-input-162-1fcd14a4efb6>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df2['instructions'] = df2['instructions'].apply(preprocess_text)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vectorizing foods and find similar foods vector"
      ],
      "metadata": {
        "id": "X5CMskYTdoJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hazm import Normalizer, word_tokenize, stopwords_list\n",
        "import string\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.metrics import jaccard_score\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Calculate similarity between food descriptions using Word2Vec model\n",
        "similarities = []\n",
        "for idx, row in df2.iterrows():\n",
        "    title_similar_words = word2vec_model.wv.most_similar(row['title'])\n",
        "    ingredients_similar_words = word2vec_model.wv.most_similar(row['ingredients'])\n",
        "    instructions_similar_words = word2vec_model.wv.most_similar(row['instructions'])\n",
        "\n",
        "\n",
        "    title_similar_words = set([word for word, _ in title_similar_words])\n",
        "    ingredients_similar_words = set([word for word, _ in ingredients_similar_words])\n",
        "    instructions_similar_words = set([word for word, _ in instructions_similar_words])\n",
        "\n",
        "\n",
        "    intersection_cnt = len(title_similar_words.intersection(ingredients_similar_words).intersection(instructions_similar_words))\n",
        "    union_cnt = len(title_similar_words.union(ingredients_similar_words).union(instructions_similar_words))\n",
        "\n",
        "    similarity = intersection_cnt / union_cnt if union_cnt != 0 else 0\n",
        "    similarities.append(similarity)\n",
        "\n",
        "specific_food_index = 3\n",
        "similarities_array = np.array(similarities)\n",
        "top_related_food_indices = similarities_array.argsort()[-6:-1][::-1]  # Exclude the food itself\n",
        "top_related_food_indices = top_related_food_indices.tolist()\n",
        "\n",
        "# Print top 5 related foods\n",
        "top_related_foods = df2.iloc[top_related_food_indices]['title']\n",
        "print(top_related_foods)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKUCkrYVlNBc",
        "outputId": "e9fd99cf-4ad7-4249-8e8c-69a4af8d8675"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21    [هویج, پلو, تن, ماهی]\n",
            "14          [ته, چین, میگو]\n",
            "13     [ته, چین, قارچ, مرغ]\n",
            "4            [ته, چین, لبو]\n",
            "6           [ته, چین, گوشت]\n",
            "Name: title, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**پشنهاد غذا بر اساس مواد اولیه آنها **\n",
        "\n",
        "###**کاربر می تواند با وارد کردن مواد اولیه اش یک دستور پخت برای غذا دریافت کند **\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xop6YIMKPO34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del df2\n",
        "df2 = data.head(25)\n",
        "df2.drop('img1', axis=1,inplace=True)\n",
        "\n",
        "!wget --header 'Host: drive.usercontent.google.com' --user-agent 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:123.0) Gecko/20100101 Firefox/123.0' --header 'Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8' --header 'Accept-Language: en-US,en;q=0.5' --referer 'https://drive.google.com/' --header 'Cookie: NID=512=cxpFxcxYHtJXDTmVhW22poOdBMX9hZOmEPPNZxsQdNcz2d2V1IyKxV-lWz-eWx7IsMed5oNhGthbSBJz4wG0iGjnyf1BHWmZ98zOpRSiXCXEBxvnSGoQgmBnMl1MWvDfSB4k_XpFtIQFXPqhQ_EmiGufwZ458QAccgfwI_O3UU8tNwUSD6Cm3e1rvUMGnjHLV9CvgqAsBg; AEC=Ae3NU9NQkR2a7Ndqa_1t-Ywer6Dn3nE8ksvX3Z5_w5yqQ5yZWkhFKehM7Q; 1P_JAR=2024-03-06-06' --header 'Upgrade-Insecure-Requests: 1' --header 'Sec-Fetch-Dest: document' --header 'Sec-Fetch-Mode: navigate' --header 'Sec-Fetch-Site: same-site' 'https://drive.usercontent.google.com/download?id=1Q3JK4NVUC2t5QT63aDiVrCRBV225E_B3&export=download' --output-document 'pos_tagger.model'\n",
        "!pip install hazm\n",
        "\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "import warnings\n",
        "import gensim\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from configparser import ConfigParser\n",
        "from functools import reduce\n",
        "from gensim.models import Doc2Vec\n",
        "from hazm import Normalizer\n",
        "import string\n",
        "from hazm import *"
      ],
      "metadata": {
        "id": "lc9JzC_DPrMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = data.head(25)\n",
        "keyword_count = 50\n",
        "stopwords = ['تعداد','دانه' , 'لیتر',  'مقدار'  , 'اندازه لازم' ,'خرد', 'متوسط' , 'به' ,'عدد',  'استکان' ,'نفر', 'پیمانه','قالب','ورق','رنده','کیلو', 'لیوان', 'میزان دلخواه' , 'میزان' , 'قاشق',  'در', 'برای', 'با', 'به', 'از', 'و', 'را', 'که', 'این', 'آن', 'یا', 'هم', 'تا', 'هر','یک' 'دو','سه','چهار','پنج','شش',',پیمانه','ق غ','عدد','قاشق چایخوری','حبه','گرم', 'عدد','بزرگ','کوچک','میزان دلخواه']\n",
        "model_path = '/content/pos_tagger.model'\n",
        "tagger = POSTagger(model = model_path)"
      ],
      "metadata": {
        "id": "9ZzOKdoj8Har"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracting ingredient collection's name based on the given grammar"
      ],
      "metadata": {
        "id": "4uqBMZLseMn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grammers = [\n",
        "\"\"\"\n",
        "NP:\n",
        "        {<NOUN,EZ>?<NOUN.*>}    # Noun(s) + Noun(optional)\n",
        "\"\"\"\n",
        "]\n",
        "\n",
        "def extract_candidates(tagged, grammer):\n",
        "    keyphrase_candidate = set()\n",
        "    np_parser = nltk.RegexpParser(grammer)\n",
        "    trees = np_parser.parse_sents(tagged)\n",
        "    for tree in trees:\n",
        "        for subtree in tree.subtrees(filter=lambda t: t.label() == 'NP'):\n",
        "            keyphrase_candidate.add(' '.join(word for word, tag in subtree.leaves()))\n",
        "    keyphrase_candidate = {kp for kp in keyphrase_candidate if len(kp.split()) <= 25}\n",
        "    keyphrase_candidate = list(keyphrase_candidate)\n",
        "    return keyphrase_candidate"
      ],
      "metadata": {
        "id": "2oU4mVsu8tfi"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Delete stopwords and apply other preprocessing operation on ingredients list"
      ],
      "metadata": {
        "id": "r_mb010hecg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_stopwords(token, stoplist):\n",
        "    words = token.split()\n",
        "    cleaned_words = [word if word not in stoplist else '' for word in words]\n",
        "    cleaned_token = ' '.join(cleaned_words)\n",
        "    return cleaned_token\n",
        "\n",
        "\n",
        "def preprocess_tokens(tokens, stoplist):\n",
        "    normalizer = Normalizer()\n",
        "    cleaned_tokens = [token.replace(',', '').replace('\\u200c', '').replace('=', '') for token in tokens if token not in string.punctuation and token != '\\u200c']\n",
        "    cleaned_tokens = [normalizer.normalize(token) for token in cleaned_tokens]\n",
        "    cleaned_tokens = [token for token in cleaned_tokens if token not in stoplist]\n",
        "    cleaned_tokens = [replace_stopwords(token, stoplist) for token in cleaned_tokens]\n",
        "    cleaned_tokens = [token.strip() for token in cleaned_tokens if token not in stoplist and len(token) >= 3]\n",
        "    return cleaned_tokens"
      ],
      "metadata": {
        "id": "iD2yUIh_8kt_"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index, row in df2.iterrows():\n",
        "    text = row['ingredients']\n",
        "\n",
        "    normalizer = Normalizer()\n",
        "    normalize_text = normalizer.normalize(text)\n",
        "    tokenize_text = []\n",
        "    for sentence in sent_tokenize(normalize_text):\n",
        "      words = word_tokenize(sentence)\n",
        "      words = [word for word in words if word not in stopwords]\n",
        "      tokenize_text.append(words)\n",
        "\n",
        "    token_tag_list = tagger.tag_sents(tokenize_text)\n",
        "\n",
        "\n",
        "    for grammer in grammers:\n",
        "      all_candidates = set()\n",
        "      all_candidates.update(extract_candidates(token_tag_list, grammer))\n",
        "      all_candidates = np.array(list(all_candidates))\n",
        "\n",
        "    cleaned_tokens = preprocess_tokens(all_candidates, stopwords)\n",
        "    df2.at[index, 'cleaned_ingredients'] = str(set(cleaned_tokens))\n",
        "    print(index , \" : \" , cleaned_tokens)"
      ],
      "metadata": {
        "id": "Avnudh8xPaqv",
        "outputId": "36eb2fbd-69d8-4c05-c238-90ba2cbd432c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-144-97f09ca7d09d>:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df2.at[index, 'cleaned_ingredients'] = str(set(cleaned_tokens))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0  :  ['کشمش', 'گوشت گوسفند', 'صورت تمایل', 'گوساله مرغ', ' روغن', 'پلویی', 'قیسی زردآلو', 'گردو', 'پیازداغ', 'همراهی برنج', 'برنج', 'فلفل']\n",
            "1  :  ['غ گوشت', 'کشمش', ' آب', 'بلغورگندم', 'پلویی', 'سیبزمینی', 'زردچوبه', 'روغن', 'قیمه', 'طلایی', 'رب گوجهفرنگی', 'پیاز', ' نخود', 'هویج', 'نمک', 'فلفل']\n",
            "2  :  ['کشمش', 'دسرها', 'چلو', 'کرده', 'قرار', 'ادویه', 'تزیین', 'مرغ', 'مناسبند فریزر', 'گرد', 'زرشک', 'هسته', 'بادام', 'ترد', 'لواش', 'صورت تمایل', 'آلو', 'کته سفره', 'بیدانه لواش', 'برنج', 'مثلثی', ' بادام', 'مورد برنج', 'اسفناج', 'دادن فر', 'سینه', 'پیاز', 'پوست', 'نمک فلفل', 'تفت', 'کره روغن', 'نکات', 'قالبها', 'بخارا', 'خلال', 'حیوانی زعفران', 'یوفکا', 'خونه']\n",
            "3  :  ['تزیین', 'کشمش', 'گوشت', ' زعفران', 'غذا', 'خوری', 'گرد لیمو', 'روغن', 'برنج', 'پیاز', 'فلفل', ' نمک']\n",
            "4  :  ['فیله ', 'تخممرغ', 'آب لبو', ' مرغ', 'لبو', ' روغن', 'وکره', 'پر رتگ', 'پیاز پیاز', 'زعفران', 'برنج', 'ریشه']\n",
            "5  :  [' ماست', 'دلخواه نمک', 'مرزه', 'زرشک', 'زردچوبه', 'تخممرغ', 'ترخان', 'برنج', 'غذاخوری', 'زعفران', 'پیاز', 'روغن کانولا', 'سینه مرغ', 'فلفل', 'لوبیاسبز']\n",
            "6  :  ['ادویه', 'سماغ', 'چوبه', 'نمک فلفل', ' پیاز', 'گوشت گوسفندی', ' کشمش', 'دلخواه مواد', 'برنج', 'پودر زیره', 'پودر پیاز', 'گرد غوره']\n",
            "7  :  ['غذاخوری', ' نمک', 'زردچوبه', ' برنج', 'فیله مرغ', 'غذاخوری نخود', 'تکه', 'زعفران', 'فلفل', 'پیاز']\n",
            "8  :  ['مرغ', 'زردچوبه', 'کشمش', ' فیله', 'غذاخوری', 'نمک', 'پلویی', 'ساقه کرفس', 'برنج', 'پیاز', 'فلفل']\n",
            "9  :  ['مرغ', 'ریش', 'نمک فلفل', ' سیبزمینی', 'پیازسرخ', 'هویج', 'خلال', 'آبکش', 'زعفران', 'برنج', ' سینه']\n",
            "10  :  ['لازم برنج', 'نمک فلفل', 'چای\\u200cخوری', 'آبکش', 'زعفران', 'سیبزمینی']\n",
            "11  :  ['مرغ', 'نمک فلفل', ' زعفران', ' کره', 'تخممرغ', 'روغن مایع', 'آلو', 'اسفناج', 'پیاز']\n",
            "12  :  ['جعفری', ' ماست', 'گوشت', ' زعفران', 'تخممرغ', 'مرباخوری', 'تره', ' سبزی', 'برنج']\n",
            "13  :  ['پیاز', ' زعفران', 'ماست', 'زردهی تخم', 'روغنبه', 'مرغ', 'زرشک', 'برنج آبکش', 'قارچ']\n",
            "14  :  ['ادویه', 'عددمتوسط گوجهفرنگی', 'وپیاز', 'میگو', 'عددسفت وقرمز', 'شویدگردلیموروغن', 'نمک', ' سیبزمینی', 'برنج']\n",
            "15  :  ['آلبالو', ' روغن', ' ماست', 'بسته ذائقه', 'سوپخوری', 'گوشت', ' زرده', 'تخممرغ', 'مایع', 'نمک فلفل', 'شکر', 'کره', 'زعفران', 'چایخوری', 'برنج', 'پیاز']\n",
            "16  :  ['نواری', 'مرباخوری', 'شربت آلبالو', ' آلبالو', 'ژولین', 'هسته', 'زردچوبه', 'روغن مایع', 'باریک گوجهفرنگی', 'چای', 'برنج', ' آبلیمو', 'مرغ', 'سینه', 'زعفران', 'سوپ\\u200cخوری', 'پیاز', 'نگینی', 'نمک فلفل', 'توضیح', 'شربت آلبالو', 'هویج', 'نمک', 'حسب سلیقه', 'شکر', 'طرز تهیه', 'ران']\n",
            "17  :  ['نمک زعفران', 'مرغ', 'زرشک', 'کشمش', 'تعدادلازم پیاز', 'پلو', 'غذا', 'آبلیمو', 'ق غذاخوری', 'برنج', ' گلاب', 'سینه ران', 'فلفل', 'خوری']\n",
            "18  :  ['مرغ', 'روغن:۳', 'لازم آب', ' گوجه', 'یککاسه', 'زردچوبه فلفل', 'غذاخوری', 'نمک', 'برنج', ' پیاز', 'رب گوجهفرنگی']\n",
            "19  :  ['کشمش', 'انار', ' گشنیز', 'خلال پسته', 'مربا', ' زعفران', ' پودر', 'پلویی', 'تخم گشنیز', 'برنج', 'خوری']\n",
            "20  :  ['لازم نمک', ' روغن', 'کره', 'لازم شیر', 'برنج']\n",
            "21  :  ['ماهی', ' سیبزمینی', 'هویج', 'برنج', 'نگینی', 'نگینی', 'سیبزمینی']\n",
            "22  :  ['پسته', 'مرغ', 'مغز گردو', 'خلال', 'مقداری', 'پوست پرتقال', 'برنج', 'نارنگی', 'زرشک', 'زعفران', 'قرمز', 'صورت نیاز', 'نمک', 'پیاز', 'چایخوری', 'شکر', 'خلال بادام', 'کشمش', 'زردچوبه']\n",
            "23  :  ['صورت', 'جعفری', 'نمک فلفل', 'پیاز:۱', 'درصورت تمایل', 'زیره ادویه', ' گوشت', 'لازم سماق', 'برنج', 'فلفل']\n",
            "24  :  ['غذاخوری', 'اسلایس', 'کرده', 'مرباخوری', 'قاچ', 'خیار', 'میلی', 'پاپریکا', 'سویا', 'روغن', 'تزین', 'برنج', 'زنجبیل', 'رشته', 'غذاخوری', 'چایخوری', ' قارچ', 'پیاز', 'پودر فلفل', 'کلم رشته', 'هویج مکعب', 'رب گوجه', 'فلفل']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### find top 5 related foods based on ingredients of foods\n",
        "### here we ar searching for a food based on it's ID"
      ],
      "metadata": {
        "id": "ysSYH69He-Ca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_similar_foods(food_id, df2):\n",
        "    input_food_ingredients = set(df2[df2['id'] == food_id]['cleaned_ingredients'].iloc[0])\n",
        "    # Calculate the intersection between the cleaned ingredients of the given food and all other foods\n",
        "    df2['intersection_count'] = df2['cleaned_ingredients'].apply(lambda x: len(input_food_ingredients.intersection(set(x))))\n",
        "    # Sort foods by intersection count and retrieve the top 5 related foods\n",
        "    similar_foods = df2[df2['id']-1 != food_id].nlargest(5, 'intersection_count')[['id', 'title', 'cleaned_ingredients']]\n",
        "    return similar_foods\n",
        "\n",
        "food_id = 2\n",
        "similar_foods = retrieve_similar_foods(food_id, df2)\n",
        "\n",
        "# Show the food names related to the top 5 related foods\n",
        "print(\"--> related foods for \" + df2['title'][food_id] + \" are : \")\n",
        "print(similar_foods[['id', 'title']])\n",
        "\n",
        "user_in = input(\"please enter ingredients you have to search suitable food : \")\n",
        "retrieve_similar_foods(food_id, df2):"
      ],
      "metadata": {
        "id": "U08KkYt81qz_",
        "outputId": "a8039a3b-36ba-41e3-d135-63b357b4befa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> related foods for پلو پرده ای اسفناج با مرغ are : \n",
            "    id              title\n",
            "1    2  دمی بلغور با قیمه\n",
            "16  17  آلبالو پلو با مرغ\n",
            "15  16      ته چین آلبالو\n",
            "17  18         ته چین مرغ\n",
            "22  23           مرصع پلو\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-148-2419004a17bc>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df2['intersection_count'] = df2['cleaned_ingredients'].apply(lambda x: len(input_food_ingredients.intersection(set(x))))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####user can enter his/her ingredient list and get some recommendation from system"
      ],
      "metadata": {
        "id": "gxgFMVMAfI_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_similar_foods(entered_ingredients, df2):\n",
        "    entered_ingredients = set(preprocess_tokens(entered_ingredients, stopwords))\n",
        "\n",
        "    # Calculate the intersection between the entered ingredients and the cleaned ingredients of all foods\n",
        "    df2['intersection_count'] = df2['cleaned_ingredients'].apply(lambda x: len(entered_ingredients.intersection(set(x))))\n",
        "\n",
        "    # Sort foods by intersection count and retrieve the top 5 related foods\n",
        "    similar_foods = df2.nlargest(5, 'intersection_count')[['id', 'title', 'cleaned_ingredients']]\n",
        "\n",
        "    return similar_foods\n",
        "\n",
        "entered_ingredients = ['لازم نمک', ' روغن', 'کره', 'لازم شیر', 'برنج']\n",
        "similar_foods_by_ingredients = retrieve_similar_foods(entered_ingredients, df2)\n",
        "print(\"--> Suitable foods based on the provided ingredients are : \")\n",
        "print(similar_foods_by_ingredients[['id', 'title']])\n"
      ],
      "metadata": {
        "id": "XObGuBYARm1O",
        "outputId": "c60e38fa-3cb9-4e55-cd24-1086a4b8b182",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> Suitable foods based on the provided ingredients are : \n",
            "   id                      title\n",
            "0   1            پلو مطنجنه هندی\n",
            "1   2          دمی بلغور با قیمه\n",
            "2   3  پلو پرده ای اسفناج با مرغ\n",
            "3   4      ته چین گوشت (گرمساری)\n",
            "4   5                 ته چین لبو\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-155-d5313a528b3e>:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df2['intersection_count'] = df2['cleaned_ingredients'].apply(lambda x: len(entered_ingredients.intersection(set(x))))\n"
          ]
        }
      ]
    }
  ]
}